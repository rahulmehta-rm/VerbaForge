{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7105c476-5664-4fce-b3f1-46d5543efcaa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Group BackBenchers\n",
    "    Shubham Agrawal C0911596\n",
    "    Tanmay Sharma C0912911  \n",
    "    Rahul Mehta C0910406\n",
    "    Hardik C0913846 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f8eb3-dd83-414c-ae59-bd444abd9a4c",
   "metadata": {},
   "source": [
    "# TeleGram Bot URL: https://t.me/VerbaForge_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a16edb2-72d7-4123-9439-c8957f87feb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rahul.RA01\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing all necessary libraries\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from telegram import Update\n",
    "from telegram.ext import Application, CommandHandler, MessageHandler, filters\n",
    "from tensorflow.keras.models import load_model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import docx\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "from textstat import flesch_reading_ease\n",
    "import pdfplumber\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad774e4-0253-463c-bf67-b1099a6621c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid model and embedding model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#Load the trained hybrid model and the SentenceTransformer model\n",
    "hybrid_model = load_model('hybrid_model.keras')\n",
    "embedding_model = SentenceTransformer('embedding_model')\n",
    "\n",
    "print(\"Hybrid model and embedding model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739c23fb-dd7d-4345-aa4f-3e85ba3dd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Cohere client\n",
    "cohere_client = cohere.Client('Use API')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94898ad1-9d3e-4fec-8330-85b491d0e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the text cleaning function \n",
    "def clean_text(text, update=None):\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #converting to lowercase\n",
    "    text = text.lower()\n",
    "    #tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "    total_words = len(words)\n",
    "    \n",
    "    #progress bar \n",
    "    processed_words = []\n",
    "    for word in tqdm(words, desc=\"Processing words\", total=total_words, ncols=100):\n",
    "        if word.isalnum() and word not in stop_words:\n",
    "            processed_words.append(lemmatizer.lemmatize(word))\n",
    "        if update and len(processed_words) % 100 == 0:  #update every 100 words\n",
    "            update.message.reply_text(f\"Processing {len(processed_words)} words...\")\n",
    "\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232f2e1b-e00a-411d-ad77-37a0390c31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function for preprocessing new essays\n",
    "def preprocess_essay(essay, embedding_model, update=None):\n",
    "    #cleaning the essay\n",
    "    cleaned_text = clean_text(essay, update)\n",
    "    \n",
    "    #extracting features\n",
    "    num_sentences = len(sent_tokenize(essay))    #number of sentences in the essay\n",
    "    words = word_tokenize(essay)                  #tokenize essay into words\n",
    "    num_words = len(words)\n",
    "    avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0      #avg sentence length\n",
    "    punctuation_density = sum(1 for char in essay if char in string.punctuation) / len(words) if num_words > 0 else 0       #punction density\n",
    "    vocab_richness = len(set(cleaned_text.split())) / len(cleaned_text.split()) if len(cleaned_text.split()) > 0 else 0     #vaocab richness\n",
    "    readability = flesch_reading_ease(cleaned_text)      #readability score\n",
    "\n",
    "    #extracting embeddings\n",
    "    embedding = embedding_model.encode([cleaned_text])[0]\n",
    "\n",
    "    #combining features into the correct format\n",
    "    features = np.array([[num_sentences, avg_sentence_length, punctuation_density, vocab_richness, readability]])\n",
    "    return embedding, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cfaae8b-9668-4aec-b5dc-9966b7c03345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for extracting text from PDF file\n",
    "def extract_text_from_pdf(file_path, update):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        text = \"\"                         #storing extracted data\n",
    "        total_pages = len(pdf.pages)      #getting total pages in PDF\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text += page.extract_text()         #extract text from the current page\n",
    "            if (i + 1) % 2 == 0 and update:     #send progress update every 2 pages\n",
    "                update.message.reply_text(f\"Extracting page {i + 1}/{total_pages}...\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c09c9c70-563c-4c92-a42a-334fe94f4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract text from Word document (.docx)\n",
    "def extract_text_from_word(file_path, update):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = \"\"\n",
    "    total_paragraphs = len(doc.paragraphs)\n",
    "    for i, para in enumerate(doc.paragraphs):\n",
    "        text += para.text               #append the text of the current paragraph to the accumulated text\n",
    "        if (i + 1) % 5 == 0 and update:  #send progress update every 5 paragraphs\n",
    "            update.message.reply_text(f\"Extracting paragraph {i + 1}/{total_paragraphs}...\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729339c5-6ab9-450a-9ace-465bdae4d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for handling uploaded files\n",
    "async def handle_file(update: Update, context):\n",
    "    #retrieving file details from the user's message\n",
    "    file = update.message.document\n",
    "    file_id = file.file_id\n",
    "    file_name = file.file_name\n",
    "    file_extension = file_name.split('.')[-1].lower()  #extracting file extension\n",
    "\n",
    "    #downloading the file\n",
    "    new_file = await context.bot.get_file(file_id)\n",
    "    file_path = f\"temp.{file_extension}\"      # save the file with a temporary name and extension\n",
    "    await new_file.download_to_drive(file_path)\n",
    "\n",
    "    #extracting text based on file type\n",
    "    if file_extension == \"pdf\":\n",
    "        text = extract_text_from_pdf(file_path, update)     #extracting text from PDF\n",
    "\n",
    "    elif file_extension == \"docx\":\n",
    "        text = extract_text_from_word(file_path, update)    #extracting from word document\n",
    "    elif file_extension == \"txt\":\n",
    "        with open(file_path, 'r') as f:                     #read text from plain text file           \n",
    "            text = f.read()\n",
    "    else:\n",
    "        await update.message.reply_text(\"Unsupported file type. Please upload a PDF, DOCX, or TXT file.\")\n",
    "        #notify the user about unsupported file types\n",
    "        os.remove(file_path)         #clean the temporary file\n",
    "        return\n",
    "\n",
    "    #preprocess the extracted text and predict the score\n",
    "    embedding, features = preprocess_essay(text, embedding_model, update)  #extract features,embeddings\n",
    "    embedding = np.array([embedding])  #wrap the embedding into a batch of size 1\n",
    "    predicted_score = hybrid_model.predict([embedding, features])     #predicting the essay score\n",
    "\n",
    "    #send the predicted score\n",
    "    await update.message.reply_text(f\"Score for the essay out of 6: {predicted_score[0][0]:.2f}\")\n",
    "\n",
    "    #ask if feedback is needed by the user\n",
    "    await update.message.reply_text(\"Do you want feedback on the essay? Type 'feedback' to receive it.\")\n",
    "\n",
    "    #saving text for feedback generation\n",
    "    context.user_data['last_essay'] = text\n",
    "\n",
    "    #clean up the file\n",
    "    os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad9b1d14-f36f-4fae-8e56-64078174f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for generating feedback\n",
    "async def generate_feedback(update: Update, context):\n",
    "    if 'last_essay' not in context.user_data:         #checking if essay is available for feedback\n",
    "        await update.message.reply_text(\"No essay found for feedback. Please upload an essay or provide text input first.\")\n",
    "        return\n",
    "\n",
    "    essay = context.user_data['last_essay']        #retrieving essay from user data\n",
    "\n",
    "    try:\n",
    "        #creating the prompt for detailed feedback\n",
    "        prompt = (\n",
    "            \"Provide detailed and constructive feedback for the following essay. Include:\\n\"\n",
    "            \"- Strengths of the essay.\\n\"\n",
    "            \"- Areas for improvement in content, structure, and grammar.\\n\"\n",
    "            \"- Suggestions for enhancing clarity and readability.\\n\\n\"\n",
    "            f\"Essay: {essay}\"\n",
    "        )\n",
    "        \n",
    "        #generate feedback using Cohere API\n",
    "        response = cohere_client.generate(\n",
    "            model='command',\n",
    "            prompt=prompt,\n",
    "            max_tokens=500  #we can adjust this if more or less feedback is needed\n",
    "        )\n",
    "        feedback = response.generations[0].text.strip()\n",
    "\n",
    "        #send the feedback\n",
    "        await update.message.reply_text(f\"Here is the feedback for your essay:\\n{feedback}\")\n",
    "    except Exception as e:\n",
    "        #handling errors during feedback generation\n",
    "        await update.message.reply_text(\"An error occurred while generating feedback. Please try again later.\")\n",
    "        print(f\"Error: {e}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43decc22-e558-4801-a481-083585f16424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to handle greetings\n",
    "async def handle_greeting(update: Update, context):\n",
    "    await update.message.reply_text(\"Hello! Hope you are doing well. Please upload an essay to score or provide text input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee09395-eb83-4055-83ff-eab5a26305de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#command to score the essay or respond to greetings\n",
    "async def score_essay(update: Update, context):\n",
    "    user_message = update.message.text.strip().lower()   #normalize the user's input text\n",
    "\n",
    "    #defining a list of greetings\n",
    "    greetings = [\"hi\", \"hello\", \"hey\", \"greetings\", \"good morning\", \"good evening\", \"good afternoon\"]\n",
    "\n",
    "    #check if the input is a greeting\n",
    "    if user_message in greetings:\n",
    "        await handle_greeting(update, context)     #handle greeting msg\n",
    "    elif user_message == \"feedback\":\n",
    "        await generate_feedback(update, context)   #handling feedback request\n",
    "    else:\n",
    "        #if not a greeting or feedback request, treat the input as an essay\n",
    "        new_essay = update.message.text\n",
    "        #preprocessing the essay for feature and embedding extraction\n",
    "        embedding, features = preprocess_essay(new_essay, embedding_model, update)\n",
    "        embedding = np.array([embedding])  #wrap the embedding into a batch of size 1\n",
    "\n",
    "        #predicting the score using the hybrid model\n",
    "        predicted_score = hybrid_model.predict([embedding, features])\n",
    "        await update.message.reply_text(f\"Predicted Score for the essay: {predicted_score[0][0]:.2f}\")\n",
    "\n",
    "        #saving the essay for feedback generation\n",
    "        context.user_data['last_essay'] = new_essay\n",
    "        await update.message.reply_text(\"Do you want feedback on the essay? Type 'feedback' to receive it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f87cd9-7157-48e7-abaa-5fc97f2037c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Processing words:   0%|                                          | 1/1105 [00:07<2:19:13,  7.57s/it]C:\\Users\\rahul.RA01\\AppData\\Local\\Temp\\ipykernel_20312\\874017790.py:22: RuntimeWarning: coroutine 'Message.reply_text' was never awaited\n",
      "  update.message.reply_text(f\"Processing {len(processed_words)} words...\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Processing words: 100%|████████████████████████████████████████| 1105/1105 [00:07<00:00, 145.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Processing words: 100%|███████████████████████████████████████| 668/668 [00:00<00:00, 147633.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Processing words: 100%|████████████████████████████████████████| 439/439 [00:00<00:00, 97043.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rahul.RA01/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Processing words: 100%|███████████████████████████████████████| 579/579 [00:00<00:00, 163041.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n"
     ]
    }
   ],
   "source": [
    "#function for starting the bot\n",
    "async def main():\n",
    "    #Telegram bot token\n",
    "    bot_token = 'Use api'\n",
    "\n",
    "    #creating an application object and initialzing with bot's token\n",
    "    application = Application.builder().token(bot_token).build()\n",
    "\n",
    "    #command to start the bot\n",
    "    application.add_handler(CommandHandler(\"start\", handle_greeting))\n",
    "\n",
    "    #handle text messages (greetings or essays)\n",
    "    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, score_essay))\n",
    "\n",
    "    #handle file uploads (PDF, DOCX, TXT)\n",
    "    application.add_handler(MessageHandler(filters.Document.ALL, handle_file))\n",
    "\n",
    "    #starting the bot\n",
    "    await application.run_polling()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712ee24-c0fd-4830-9132-aab0ff3e731d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
